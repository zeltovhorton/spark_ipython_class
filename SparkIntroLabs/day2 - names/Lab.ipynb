{"cells":[{"cell_type":"markdown","source":["# DataFrames Python Lab"],"metadata":{}},{"cell_type":"markdown","source":["**Remember**: In the notebook, you already have a `SQLContext` object, called `sqlContext`. However, if you need to create one yourself (e.g., for a non-notebook application), do it like this:\n```\n# assuming \"sc\" is some existing SparkContext object\nsqlContext = SQLContext(sc)\n```\n\n**WARNING: Don't ever do this IN the notebook!**"],"metadata":{}},{"cell_type":"markdown","source":["We've already created a Parquet table containing popular first names by gender and year, for all years between 1880 and 2014. (This data comes from the United States Social Security Administration.) We can create a DataFrame from that data, by calling `sqlContext.read.parquet()`.\n\n**NOTE**: That's the Spark 1.4 API. The API is slightly different in Spark 1.3."],"metadata":{}},{"cell_type":"code","source":["# Spark 1.4\ndf = sqlContext.read.parquet(\"dbfs:/mnt/training/ssn/names.parquet\")\n\n# Spark 1.3\n# df = sqlContext.parquetFile(\"dbfs:/mnt/training/ssn/names.parquet\")"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":["Let's cache our Social Security names DataFrame, to speed things up."],"metadata":{}},{"cell_type":"code","source":["df.cache()"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":["Let's take a quick look at the first 20 items of the data."],"metadata":{}},{"cell_type":"code","source":["df.show()"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":["You can also use the `display()` helper, to get more useful (and graphable) output:"],"metadata":{}},{"cell_type":"code","source":["display(df)"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":["Take a look at the data schema, as well. Note that, in this case, the schema was read from the columns (and types) in the Parquet table."],"metadata":{}},{"cell_type":"code","source":["df.printSchema()"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":["You can create a new DataFrame that looks at a subset of the columns in the first DataFrame."],"metadata":{}},{"cell_type":"code","source":["firstNamesDF = df.select(\"firstName\", \"year\")"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":["Then, you can examine the values in the `nameDF` DataFrame, using an action like `show()` or the `display()` helper:"],"metadata":{}},{"cell_type":"code","source":["display(firstNamesDF)"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":["You can also count the number of items in the data set..."],"metadata":{}},{"cell_type":"code","source":["firstNamesDF.count()"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":["...or determine how many distinct names there are."],"metadata":{}},{"cell_type":"code","source":["firstNamesDF.select(\"firstName\").distinct().count()"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":["Let's do something a little more complicated. Let's use the original data frame to find the five most popular names for girls born in 1980. Note the `desc()` in the `orderBy()` call. `orderBy()` (which can also be invoked as `sort()`) sorts in ascending order. `desc()` causes the sort to be in _descending_ order for the column to which `desc()` is attached."],"metadata":{}},{"cell_type":"code","source":["display(df.filter(df['year'] == 1980).\n           filter(df['gender'] == 'F').\n           orderBy(df['total'].desc(), df['firstName']).\n           select(\"firstName\").\n           limit(5))"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":["Note that we can do the same thing using the lower-level RDD operations. However, use the DataFrame operations, when possible. In general, they're more convenient. More important, though, they allow Spark to build a query plan that can be optimized through [Catalyst](https://databricks.com/blog/2015/04/13/deep-dive-into-spark-sqls-catalyst-optimizer.html)."],"metadata":{}},{"cell_type":"markdown","source":["## Joins"],"metadata":{}},{"cell_type":"markdown","source":["Let's use two views of our SSN data to answer this question: How popular were the top 10 female names of 2010 back in 1930?\n\nBefore we can do that, though, we need to define a utility function. Spark SQL doesn't support the SQL `LOWER` function. To ensure that our data matches up properly, it'd be nice to force the names to lower case before doing the match. Fortunately, it's easy to define our own `LOWER` function:"],"metadata":{}},{"cell_type":"code","source":["lower = udf(lambda s: s.lower())"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"code","source":["from pyspark.sql.functions import *"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"code","source":["lower"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"markdown","source":["Okay, now we can go to work."],"metadata":{}},{"cell_type":"code","source":["# Create a new DataFrame from the SSNA DataFrame, so that:\n# - We have a lower case version of the name, for joining\n# - We've weeded out the year\n#\n# NOTE: The aliases are necessary; otherwise, the query analyzer\n# generates false equivalences between the columns.\nssn2010 = df.filter(df['year'] == 2010).\\\n             select(df['total'].alias(\"total2010\"), \n                    df['gender'].alias(\"gender2010\"), \n                    df['firstName'].alias(\"firstName2010\"),\n                    lower(df['firstName']).alias(\"name2010\"))\n\n# Let's do the same for 1930.\nssn1930 = df.filter(df['year'] == 1930).\\\n             select(df['total'].alias(\"total1930\"), \n                    df['gender'].alias(\"gender1930\"), \n                    df['firstName'].alias(\"firstName1930\"),\n                    lower(df['firstName']).alias(\"name1930\"))\n\n# Now, let's find out how popular the top 10 New York 2010 girls' names were in 1880.\n# This join works fine in Scala, but not (yet) in Python.\n#\n# j1 = ssn2010.join(ssn1930, (ssn2010.name2010 == ssn1930.name1930) and (ssn2010.gender2010 == ssn1930.gender1930))\n\n# So, we'll do a slightly different version.\njoined = ssn2010.filter(ssn2010.gender2010 == \"F\").\\\n                 join(ssn1930.filter(ssn1930.gender1930 == \"F\"), ssn2010.name2010 == ssn1930.name1930).\\\n                 orderBy(ssn2010.total2010.desc()).\\\n                 limit(10).\\\n                 select(ssn2010.firstName2010.alias(\"name\"), ssn1930.total1930, ssn2010.total2010)\n\ndisplay(joined)"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"markdown","source":["## Assignment"],"metadata":{}},{"cell_type":"markdown","source":["In the cell below, you'll see an empty function, `top_female_names_for_year`. It takes three arguments:\n\n* A year\n* A number, _n_\n* A starting DataFrame (which will be `df`, from above)\n\nIt returns a new DataFrame that can be used to retrieve the top _n_ female names for that year (i.e., the _n_ names with the highest _total_ values). If there are multiple names with the same total, order those names alphabetically.\n\nWrite that function. To test it, run the cell _following_ the function?i.e., the one containing the `run_tests()` function. (This might take a few minutes.)"],"metadata":{}},{"cell_type":"code","source":["def top_female_names_for_year(year, n, df):\n  return df.limit(10) # THIS IS NOT CORRECT! FIX IT."],"metadata":{},"outputs":[],"execution_count":33},{"cell_type":"code","source":["# Transparent Tests\nfrom test_helper import Test\ndef test_year(year, df):\n    return [row.firstName for row in top_female_names_for_year(year, 5, df).collect()]"],"metadata":{},"outputs":[],"execution_count":34},{"cell_type":"code","source":["def run_tests():\n  Test.assertEquals(test_year(1945, df), [u'Mary', u'Linda', u'Barbara', u'Patricia', u'Carol'], 'incorrect top 5 names for 1945')\n  Test.assertEquals(test_year(1970, df), [u'Jennifer', u'Lisa', u'Kimberly', u'Michelle', u'Amy'], 'incorrect top 5 names for 1970')\n  Test.assertEquals(test_year(1987, df), [u'Jessica', u'Ashley', u'Amanda', u'Jennifer', u'Sarah'], 'incorrect top 5 names for 1987')\n  Test.assertTrue(len(test_year(1945, df)) <= 5, 'list not limited to 5 names')\n  Test.assertTrue(u'James' not in test_year(1945, df), 'male names not filtered')\n  Test.assertTrue(test_year(1945, df) != [u'Linda', u'Linda', u'Linda', u'Linda', u'Mary'], 'year not filtered')\n  Test.assertEqualsHashed(test_year(1880, df), \"2038e2c0bb0b741797a47837c0f94dbf24123447\", \"incorrect top 5 names for 1880\")\n  \nrun_tests()"],"metadata":{},"outputs":[],"execution_count":35},{"cell_type":"markdown","source":[""],"metadata":{}},{"cell_type":"markdown","source":["## Solution\n\nIf you're stuck, and you're really not sure how to proceed, feel free to check out the solution. You'll find it in the same folder as the lab."],"metadata":{}},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":38}],"metadata":{"name":"Lab","notebookId":10348},"nbformat":4,"nbformat_minor":0}
