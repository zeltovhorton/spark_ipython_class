{"cells":[{"cell_type":"markdown","source":["# SQL and DataFrames: Hands-on Exercises (Python)\n\nThis notebook contains hands-on exercises used in conjunction with the DataFrames module. Each section corresponds to a section in the lecture. Your instructor will tell you when it's time to do each section of this notebook."],"metadata":{}},{"cell_type":"markdown","source":["## Schema Inference\n\nIn this exercise, let's explore schema inference. We're going to be using a file called `people.txt`. The data is structured, but it has no self-describing schema. And, it's not JSON, so Spark can't infer the schema automatically. Let's create an RDD and look at the first few rows of the file."],"metadata":{}},{"cell_type":"code","source":["rdd = sc.textFile(\"dbfs:/mnt/databricks-corp-training/common/dataframes/people.txt\")\nfor line in rdd.take(10):\n  print line"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":["As you can see, each line consists of the same information about a person:\n\n* first name\n* middle name\n* last name\n* gender (\"M\" or \"F\")\n* birth date, in `yyyy-mm-dd` form\n* a salary\n* a United States Social Security Number\n\n(Before you get _too_ excited and run out to apply for a bunch of credit cards, the Social Security Numbers are all fake.)\n\nClearly, the file has a schema, but Spark can't figure out what it is.\n\nRead through the following code to see how we can apply a schema to the file. Then, run it, and see what happens."],"metadata":{}},{"cell_type":"code","source":["from datetime import datetime\nfrom collections import namedtuple\n\nPerson = namedtuple('Person', ['first_name', 'middle_name', 'last_name', 'gender', 'birth_date', 'salary', 'ssn'])\n\ndef map_to_person(line):\n  cols = line.split(\":\")\n  return Person(first_name  = cols[0],\n                middle_name = cols[1],\n                last_name   = cols[2],\n                gender      = cols[3],\n                birth_date  = datetime.strptime(cols[4], \"%Y-%m-%d\"),\n                salary      = int(cols[5]),\n                ssn         = cols[6])\n    \npeople_rdd = rdd.map(map_to_person)\ndf = people_rdd.toDF()\n"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":["**Question:** What could go wrong in the above code? How would you fix the problems?"],"metadata":{}},{"cell_type":"markdown","source":["Now, let's sample some of the data."],"metadata":{}},{"cell_type":"code","source":["sampledDF = df.sample(withReplacement = False, fraction = 0.02, seed = 1887348908234L)\ndisplay(sampledDF)"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":["Finally, let's run a couple SQL commands."],"metadata":{}},{"cell_type":"code","source":["df.registerTempTable(\"people\")"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["%sql SELECT * FROM people WHERE birth_date >= '1970-01-01' AND birth_date <= '1979-12-31' ORDER BY birth_date, salary"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["%sql SELECT concat(first_name, \" \", last_name) AS name, gender, year(birth_date) AS birth_year, salary FROM people WHERE salary < 50000"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":["### ![](http://i.imgur.com/RdABwEB.png) STOP HERE.\n\nLet's switch back to the slides."],"metadata":{}},{"cell_type":"markdown","source":["## select and filter (and a couple more)\n\nWe've now seen `printSchema()`, `show()`, `select()` and `filter()`. Let's take them for a test drive.\n\nFirst, let's look at the schema."],"metadata":{}},{"cell_type":"code","source":["df.printSchema()"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":["Now, let's look at `show()`."],"metadata":{}},{"cell_type":"code","source":["df.show() # show the first 20 rows of the DataFrame"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":["`show()` is a good way to get a quick feel for your data. Of course, in a Databricks notebook, the `display()` helper is better. However, if you're using `spark-shell`, the `display()` helper isn't available."],"metadata":{}},{"cell_type":"code","source":["display(df)"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":["Let's look at `select()`. Run the following cell. What does it return?"],"metadata":{}},{"cell_type":"code","source":["df.select(df[\"first_name\"], df[\"last_name\"], df[\"gender\"])"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":["**Remember**: Transformations are _lazy_. The `select()` method is a transformation.\n\nAll right. Let's look at result of a `select()` call."],"metadata":{}},{"cell_type":"code","source":["df.select(df[\"first_name\"], df[\"last_name\"], df[\"gender\"]).show(10)"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"markdown","source":["Finally, let's take a look at `filter()`, which can be used to filter data _out_ of a data set.\n\n**Question**: What the does the following code actually do?"],"metadata":{}},{"cell_type":"code","source":["df.filter(df[\"gender\"] == \"M\")"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"markdown","source":["If you're familiar with the DataFrames Scala API, you'll notice that we use double-equals (`==`) in that comparison, not the triple-equals (`===`) we use in Scala. If you switch between languages, keep that in mind.\n\n`filter()`, like `select()`, is a transformation: It's _lazy_.\n\nLet's try something a little more complicated. Let's combine two `filter()` operations with a `select()`, displaying the results."],"metadata":{}},{"cell_type":"code","source":["df2 = df.filter(df[\"gender\"] == \"M\").filter(df[\"salary\"] > 100000).select(df[\"first_name\"], df[\"last_name\"], df[\"salary\"])\ndisplay(df2)"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"markdown","source":["### ![](http://i.imgur.com/RdABwEB.png) STOP HERE.\n\nLet's switch back to the slides."],"metadata":{}},{"cell_type":"markdown","source":["## orderBy, groupBy and alias"],"metadata":{}},{"cell_type":"markdown","source":["Up in the first section of this notebook, we ran this SQL statement:\n\n```\nSELECT * FROM people WHERE birthDate >= '1970-01-01' AND birthDate <= '1979-12-31' ORDER BY birthDate, salary\n```\n\nLet's try that same query with the programmatic DataFrames API."],"metadata":{}},{"cell_type":"code","source":["display(\n  df.filter(df[\"birth_date\"] >= \"1970-01-01\").filter(df[\"birth_date\"] <= \"1979-12-31\").orderBy(df.birth_date, df.salary)\n)"],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"markdown","source":["There are several things to note.\n\n1. We did not have to convert the date literals (\"1970-01-01\" and \"1979-12-31\") into `datetime` objects before using them in the comparisons.\n2. We used two different ways to specify the columns: `df[\"firstName\"]` and `df.first_name`.\n\nLet's try a `groupBy()` next."],"metadata":{}},{"cell_type":"code","source":["display( df.groupBy(df[\"salary\"]) )"],"metadata":{},"outputs":[],"execution_count":33},{"cell_type":"markdown","source":["Okay, that didn't work. Note that `groupBy()` returns something of type `GroupedData`, instead of a `DataFrame`. There are other methods on `GroupedData` that will convert back to a DataFrame. A useful one is `count()`.\n\n**WARNING**: Don't confuse `GroupedData.count()` with `DataFrame.count()`. `GroupedData.count()` is _not_ an action. `DataFrame.count()` _is_ an action."],"metadata":{}},{"cell_type":"code","source":["x = df.groupBy(df[\"salary\"]).count()  # What is x?"],"metadata":{},"outputs":[],"execution_count":35},{"cell_type":"code","source":["display(x)"],"metadata":{},"outputs":[],"execution_count":36},{"cell_type":"markdown","source":["Let's add a filter and, while we're at it, rename the `count` column."],"metadata":{}},{"cell_type":"code","source":["display( x.filter(x['count'] > 1).select(\"salary\", x[\"count\"].alias(\"total\")) )"],"metadata":{},"outputs":[],"execution_count":38},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":39}],"metadata":{"name":"Python","notebookId":10401},"nbformat":4,"nbformat_minor":0}
