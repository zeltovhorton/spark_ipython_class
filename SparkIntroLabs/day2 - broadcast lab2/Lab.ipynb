{"cells":[{"cell_type":"markdown","source":["# Custom Accumulators Lab\n\nIn this lab, we'll build a couple custom Accumulator classes. (Well, we'll show you one; then you'll build one of your own.)\n\nFirst, some useful imports..."],"metadata":{}},{"cell_type":"code","source":["from pyspark import *\nimport random"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":["Let's create an accumulator that's a map. We'll use it to count words. Normally, we'd just use the typical word-count RDD transformation pattern, but this alternate approach is useful when you want to do the counting as a _side effect_ of something else you're doing."],"metadata":{}},{"cell_type":"markdown","source":["We'll need a custom accumulator class. This accumulator will be a map of `String` (word) to `Int` (word count)."],"metadata":{}},{"cell_type":"code","source":["class AccumWordCounts(AccumulatorParam):\n    def zero(self, initialValue):\n        '''\n        zero() takes an initial value and converts it (if necessary)\n        into the actual initial value for the accumulator. In this case,\n        we'll just accept the caller's initial value.\n    \n        initial - the caller's initial value.\n    \n        returns the actual initial value\n        '''\n        return {}\n\n    def addInPlace(self, d1, d2):\n        '''\n        addInPlace() is responsible for taking two maps and combining\n        them into one. Spark uses it to sum up the various node-specific\n        instances of the accumulator.\n    \n        m1  the first map\n        m2  the second map\n\n        returns the combined map\n        '''\n        keys1 = set(d1.keys())\n        keys2 = set(d2.keys())\n        common_keys = keys1 & keys2\n        unique1 = keys1 - common_keys\n        unique2 = keys2 - common_keys\n        \n        # The keys that are common between both maps must have their\n        # counts summed. The keys that are unique can just be copied\n        # to the new map.\n        common_tuples = [(k, d1[k] + d2[k]) for k in common_keys]\n        unique1_tuples = [(k, d1[k]) for k in unique1]\n        unique2_tuples = [(k, d2[k]) for k in unique2]\n        return dict(common_tuples + unique1_tuples + unique2_tuples)"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":["Next, we need to create an instance of an accumulator of this type."],"metadata":{}},{"cell_type":"code","source":["count_map = sc.accumulator({}, AccumWordCounts())"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":["Now, let's test it with a parallelized data set, which makes it easier to validate."],"metadata":{}},{"cell_type":"code","source":["rdd = sc.parallelize([\"and\", \"and\", \"then\", \"the\", \"leaves\", \"grass\", \"green\", \"leaves\"])"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":["We'll use the distributed action `foreach` to count each word. In addition, we'll convert the RDD to another RDD with upper-cased words."],"metadata":{}},{"cell_type":"code","source":["rdd2 = rdd.map(lambda word: word.upper())\n# Note that we have to add a dict here.\ndef update(word):\n  global count_map\n  count_map += {word: 1}\nrdd2.foreach(update)"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["for word in rdd2.collect():\n  print word"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":["What's the accumulator look like?"],"metadata":{}},{"cell_type":"code","source":["counts = count_map.value\nfor key in sorted(counts):\n  print \"{0} -> {1}\".format(key, counts[key])"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":["## Exercise\n\nYou're going to create an accumulator that can be used to keep track of unique occurrences of numbers. A `set` is a useful way to keep track of uniqueness."],"metadata":{}},{"cell_type":"code","source":["class AccumWordCounts(AccumulatorParam):\n    def zero(self, ...):\n      # FILL IN\n      pass\n\n    def addInPlace(self, ...):\n      # FILL IN\n      pass"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["unique_numbers = sc.accumulator(set(), AccumSet())"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["# Check the initial value of the accumulator.\nunique_numbers.value"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":["To test your accumulator, we'll use 100,000 random numbers, with some guaranteed overlap. (We'd use 1,000,000, like the Scala lab, but Python is too slow...)"],"metadata":{}},{"cell_type":"code","source":["numbers = [random.randint(0, 10000) for i in range(1, 100000)]"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"code","source":["rdd = sc.parallelize(numbers, 4)"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":["Here's where you need to update the accumulator."],"metadata":{}},{"cell_type":"code","source":["def update_set(i):\n  # FILL IN\n  pass\n\nrdd.foreach(...)"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"code","source":["print(\"Random numbers: {0}\\nUnique numbers: {1}\".format(\n  rdd.count(), len(unique_numbers.value))\n)"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":25}],"metadata":{"name":"Lab","notebookId":8629},"nbformat":4,"nbformat_minor":0}
