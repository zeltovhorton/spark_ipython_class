{"cells":[{"cell_type":"markdown","source":["<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-nd/4.0/\"><img alt=\"Creative Commons License\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by-nc-nd/4.0/88x31.png\" /></a><br />This work is licensed under a <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-nd/4.0/\">Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License</a>."],"metadata":{}},{"cell_type":"markdown","source":["# Pipelines and Logistic Regression\n \nIn this lab we'll cover transformers, estimators, evaluators, and pipelines.  We'll use transformers and estimators to prepare our data for use in a logistic regression model and will use pipelines to combine these steps together.  Finally, we'll evaluate our model.\n \nThis lab also covers creating train and test datasets using `randomSplit`, visualizing a ROC curve, and generating both `ml` and `mllib` logistic regression models.\n \nAfter completing this lab you should be comfortable using transformers, estimators, evaluators, and pipelines."],"metadata":{}},{"cell_type":"code","source":["baseDir = \"/mnt/ml-class/\"\nirisTwoFeatures = sqlContext.read.parquet(baseDir + 'irisTwoFeatures.parquet').cache()\nprint '\\n'.join(map(repr, irisTwoFeatures.take(2)))"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["display(irisTwoFeatures)"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":["#### Prepare the data\n \nTo explore our data in more detail, we're going to we pull out sepal length and sepal width and create two columns.  These are the two features found in our `DenseVector`.\n \nIn order to do this you will write a `udf` that takes in two values.  The first will be the name of the vector that we are operating on and the second is a literal for the index position.  Here are links to `lit` in the [Python](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.lit) and [Scala](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.functions$) APIs.\n \nThe `udf` will return a `DoubleType` that is the value of the specified vector at the specified index position.\n \nIn order to call our function, we need to wrap the second value in `lit()` (e.g. `lit(1)` for the second element).  This is because our `udf` expects a `Column` and `lit` generates a `Column` where the literal is the value."],"metadata":{}},{"cell_type":"code","source":["# ANSWER\nfrom pyspark.sql.functions import udf, lit\nfrom pyspark.sql.types import DoubleType\n\n# Remember to cast the value you extract from the Vector using float()\ngetElement = udf(lambda v, i: float(v[i]), DoubleType())\n\nirisSeparateFeatures = (irisTwoFeatures\n                        .withColumn('sepalLength', getElement('features', lit(0)))\n                        .withColumn('sepalWidth', getElement('features', lit(1))))\ndisplay(irisSeparateFeatures)\n"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["# TEST\nfrom test_helper import Test\nfirstRow = irisSeparateFeatures.select('sepalWidth', 'features').map(lambda r: (r[0], r[1])).first()\nTest.assertEquals(firstRow[0], firstRow[1][1], 'incorrect definition for getElement')"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":["What about using `Column`'s `getItem` method?"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.functions import col\nfrom pyspark.sql.utils import AnalysisException\n\ntry:\n    display(irisTwoFeatures.withColumn('sepalLength', col('features').getItem(0)))\nexcept AnalysisException as e:\n    print e"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":["Unfortunately, it doesn't work for vectors, but it does work on arrays."],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql import Row\narrayDF = sqlContext.createDataFrame([Row(anArray=[1,2,3]), Row(anArray=[4,5,6])])\narrayDF.show()\n\narrayDF.select(col('anArray').getItem(0)).show()\narrayDF.select(col('anArray')[1]).show()\n"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":["## Part 2"],"metadata":{}},{"cell_type":"markdown","source":["Next, let's register our function and then call it directly from SQL."],"metadata":{}},{"cell_type":"code","source":["sqlContext.udf.register('getElement', getElement.func, getElement.returnType)\nirisTwoFeatures.registerTempTable('irisTwo')"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["%sql\nselect getElement(features, 0) as sepalLength from irisTwo"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":["#### EDA and feature engineering"],"metadata":{}},{"cell_type":"markdown","source":["Let's see the ranges of our values and view their means and standard deviations."],"metadata":{}},{"cell_type":"code","source":["display(irisSeparateFeatures.describe('label', 'sepalLength', 'sepalWidth'))"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":["Our features both take on values from -1.0 to 1.0, but have different means and standard deviations.  How could we standardize our data to have zero mean and unit standard deviations?  For this task we'll use the `ml` estimator `StandardScaler`.  Feature transformers (which are sometimes estimators) can be found in [pyspark.ml.feature](http://spark.apache.org/docs/latest/api/python/pyspark.ml.html#module-pyspark.ml.feature) for Python or [org.apache.spark.ml.feature](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.feature.package) for Scala.\n \nAlso, remember that the [ML Guide](http://spark.apache.org/docs/latest/ml-features.html#standardscaler) is a good place to find additional information."],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.feature import StandardScaler\nhelp(StandardScaler)"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"code","source":["standardScaler = (StandardScaler()\n                  .setInputCol('features')\n                  .setOutputCol('standardized')\n                  .setWithMean(True))\n\nprint standardScaler.explainParams()"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"code","source":["irisStandardizedLength = (standardScaler\n                          .fit(irisSeparateFeatures)\n                          .transform(irisSeparateFeatures)\n                          .withColumn('standardizedLength', getElement('standardized', lit(0))))\ndisplay(irisStandardizedLength)"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"code","source":["display(irisStandardizedLength.describe('sepalLength', 'standardizedLength'))"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"markdown","source":["What if instead we wanted to normalize the data?  For example, we might want to normalize each set of features (per row) to have length one using an \\\\( l^2 \\\\) norm.  That would cause the sum of the features squared to be one: \\\\( \\sum_{i=1}^d x_i^2 = 1 \\\\).  This is could be useful if we wanted to compare observations based on a distance metric like in k-means clustering.\n \n`Normalizer` can be found in [pyspark.ml.feature](https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.feature.Normalizer) for Python and the [org.apache.spark.ml.feature](http://spark.apache.org/docs/latest/api/scala/#org.apache.spark.ml.feature.Normalizer) package for Scala.\n \nLet's implement `Normalizer` and transform our features.  Make sure to use a `P` of 2.0 and to name the output column \"featureNorm\".  Remember that we're working with the `irisTwoFeatures` dataset."],"metadata":{}},{"cell_type":"code","source":["# ANSWER\nfrom pyspark.ml.feature import Normalizer\nnormalizer = (Normalizer()\n              .setInputCol('features')\n              .setOutputCol('featureNorm')\n              .setP(2.0))\n\nirisNormalized = normalizer.transform(irisTwoFeatures)  # Note that we're calling transform here\ndisplay(irisNormalized)"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"code","source":["# TEST\nimport numpy as np\nfirstVector = irisNormalized.select('featureNorm').map(lambda r: r[0]).first()\nTest.assertTrue(np.allclose(firstVector.norm(2.0), 1.0), 'incorrect setup of normalizer')"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"markdown","source":["## Part 3"],"metadata":{}},{"cell_type":"markdown","source":["Let's just check and see that our norms are equal to 1.0"],"metadata":{}},{"cell_type":"code","source":["l2Norm = udf(lambda v: float(v.norm(2.0)), DoubleType())\n\nfeatureLengths = irisNormalized.select(l2Norm('features').alias('featuresLength'),\n                                       l2Norm('featureNorm').alias('featureNormLength'))\ndisplay(featureLengths)"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"markdown","source":["Next, let's bucketize our features.  This will allow us to convert continuous features into discrete buckets.  This is often desirable for logistic regression which we'll be performing later in this lab.\n \nWe'll use the following splits: -infinity, -0.5, 0.0, 0.5, +infinity.  Note that in Python infinity can be represented using `float('inf')` and that in Scala `Double.NegativeInfinity` and `Double.PositiveInfinity` can be used."],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.feature import Bucketizer\n\nsplits = [-float('inf'), -.5, 0.0, .5, float('inf')]\n\nlengthBucketizer = (Bucketizer()\n              .setInputCol('sepalLength')\n              .setOutputCol('lengthFeatures')\n              .setSplits(splits))\n\nirisBucketizedLength = lengthBucketizer.transform(irisSeparateFeatures)\ndisplay(irisBucketizedLength)"],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"code","source":["widthBucketizer = (Bucketizer()\n                   .setInputCol(\"sepalWidth\")\n                   .setOutputCol(\"widthFeatures\")\n                   .setSplits(splits))\n\nirisBucketizedWidth = widthBucketizer.transform(irisBucketizedLength)\ndisplay(irisBucketizedWidth)"],"metadata":{},"outputs":[],"execution_count":32},{"cell_type":"markdown","source":["Let's combine the two bucketizers into a [Pipeline](http://spark.apache.org/docs/latest/ml-guide.html#pipeline-components) that performs both bucketizations.  A `Pipeline` is made up of stages which can be set using `setStages` and passing in a `list` of stages in Python or an `Array` of stages in Scala.  `Pipeline` is an estimator, which means it implements a `fit` method which returns a `PipelineModel`.  A `PipelineModel` is a transformer, which means that it implements a `transform` method which can be used to run the stages."],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.pipeline import Pipeline\n\npipelineBucketizer = Pipeline().setStages([lengthBucketizer, widthBucketizer])\n\npipelineModelBucketizer = pipelineBucketizer.fit(irisSeparateFeatures)\nirisBucketized = pipelineModelBucketizer.transform(irisSeparateFeatures)\n\ndisplay(irisBucketized)\n"],"metadata":{},"outputs":[],"execution_count":34},{"cell_type":"markdown","source":["Now that we have created two new features through bucketing, let's combine those two features into a `Vector` with `VectorAssembler`.  VectorAssembler can be found in [pyspark.ml.feature](https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.feature.VectorAssembler) for Python and the [org.apache.spark.ml.feature](http://spark.apache.org/docs/latest/api/scala/#org.apache.spark.ml.feature.VectorAssembler) package for Scala.\n \nSet the params of `assembler` so that both \"lengthFeatures\" and \"widthFeatures\" are assembled into a column called \"featuresBucketized\".\n \nThen, set the stages of `pipeline` to include both bucketizers and the assembler as the last stage.\n \nFinally, use `pipeline` to generate a new `DataFrame` called `irisAssembled`."],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.feature import VectorAssembler\npipeline = Pipeline()\nassembler = VectorAssembler()\n\nprint assembler.explainParams()\nprint '\\n',pipeline.explainParams()"],"metadata":{},"outputs":[],"execution_count":36},{"cell_type":"code","source":["# ANSWER\n# Set assembler params\n(assembler\n .setInputCols(['lengthFeatures', 'widthFeatures'])\n .setOutputCol('featuresBucketized'))\n\npipeline.setStages([lengthBucketizer, widthBucketizer, assembler])\nirisAssembled = pipeline.fit(irisSeparateFeatures).transform(irisSeparateFeatures)\ndisplay(irisAssembled)"],"metadata":{},"outputs":[],"execution_count":37},{"cell_type":"code","source":["# TEST\nfrom pyspark.mllib.linalg import Vectors\nfirstAssembly = irisAssembled.select('lengthFeatures', 'widthFeatures', 'featuresBucketized').first()\nTest.assertTrue(all(firstAssembly[2].toArray() == [firstAssembly[0], firstAssembly[1]]),\n                'incorrect value for column featuresBucketized')"],"metadata":{},"outputs":[],"execution_count":38},{"cell_type":"markdown","source":["## Part 4"],"metadata":{}},{"cell_type":"markdown","source":["#### Logistic Regression"],"metadata":{}},{"cell_type":"markdown","source":["First let's look at our data by label."],"metadata":{}},{"cell_type":"code","source":["display(irisSeparateFeatures.groupBy('label').count().orderBy('label'))"],"metadata":{},"outputs":[],"execution_count":42},{"cell_type":"markdown","source":["Let's build a model that tries to differentiate between the first two classes."],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.functions import col\nirisTwoClass = irisSeparateFeatures.filter(col('label') < 2)\ndisplay(irisTwoClass.groupBy('label').count().orderBy('label'))"],"metadata":{},"outputs":[],"execution_count":44},{"cell_type":"markdown","source":["Next, we'll split our dataset into test and train sets."],"metadata":{}},{"cell_type":"code","source":["irisTest, irisTrain = irisTwoClass.randomSplit([.25, .75], seed=0)\n\n# Cache as we'll be using these several times\nirisTest.cache()\nirisTrain.cache()\n\nprint 'Items in test datset: {0}'.format(irisTest.count())\nprint 'Items in train dataset: {0}'.format(irisTrain.count())"],"metadata":{},"outputs":[],"execution_count":46},{"cell_type":"markdown","source":["And now let's build our logistic regression model.  LogisticRegression can be found in [pyspark.ml.classification](http://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.classification.LogisticRegression) for Python and the [org.apache.spark.ml.classification](http://spark.apache.org/docs/latest/api/scala/#org.apache.spark.ml.classification.LogisticRegression) package for Scala.  The ML Guide also has a nice overview of [logistic regression](http://spark.apache.org/docs/latest/ml-classification-regression.html#logistic-regression).\n \nMake sure to set the featuresCol to \"featuresBucketized\", the regParam to 0.0, the labelCol to \"label\", and the maxIter to 1000.\n \nAlso, set the pipeline stages to include the two bucketizers, assembler, and logistic regression."],"metadata":{}},{"cell_type":"code","source":["# ANSWER\nfrom pyspark.ml.classification import LogisticRegression\n\nlr = (LogisticRegression()\n      .setFeaturesCol('featuresBucketized')\n      .setRegParam(0.0)\n      .setLabelCol('label')\n      .setMaxIter(1000))\n\npipeline.setStages([lengthBucketizer, widthBucketizer, assembler, lr])\n\npipelineModelLR = pipeline.fit(irisTrain)\n\nirisTestPredictions = (pipelineModelLR\n                       .transform(irisTest)\n                       .cache())\ndisplay(irisTestPredictions)"],"metadata":{},"outputs":[],"execution_count":48},{"cell_type":"code","source":["irisTestPredictions.select(\"probability\").first()[0][0]"],"metadata":{},"outputs":[],"execution_count":49},{"cell_type":"code","source":["# TEST\nTest.assertTrue(sum(irisTestPredictions.select(\"probability\").first()[0]) > .99,\n                'incorrect build of the lr model')"],"metadata":{},"outputs":[],"execution_count":50},{"cell_type":"markdown","source":["## Part 5"],"metadata":{}},{"cell_type":"code","source":["print pipelineModelLR.stages\nprint '\\n{0}'.format(pipelineModelLR.stages[-1].weights)"],"metadata":{},"outputs":[],"execution_count":52},{"cell_type":"markdown","source":["Leaving our features to range from 0 to 3 means that a value of 2 has twice the impact in our model than a value of 1.  Since these buckets were based on increasing numeric values this is not unreasonable; however, we might want to convert each of these values to a dummy feature that takes on either a 0 or 1 corresponding to whether the value occurs.  This allows the model to measure the impact of the occurrences of the individual values and allows for non-linear relationships.\n \nTo do this we'll use the `OneHotEncoder` estimator."],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.feature import OneHotEncoder\n\noneHotLength = (OneHotEncoder()\n                .setInputCol('lengthFeatures')\n                .setOutputCol('lengthOneHot'))\n\npipeline.setStages([lengthBucketizer, widthBucketizer, oneHotLength])\n\nirisWithOneHotLength = pipeline.fit(irisTrain).transform(irisTrain)\ndisplay(irisWithOneHotLength)"],"metadata":{},"outputs":[],"execution_count":54},{"cell_type":"code","source":["irisWithOneHotLength.select('lengthOneHot').first()"],"metadata":{},"outputs":[],"execution_count":55},{"cell_type":"markdown","source":["Create a `OneHotEncoder` for width as well, and combine both encoders together into a `featuresBucketized` column."],"metadata":{}},{"cell_type":"code","source":["oneHotWidth = (OneHotEncoder()\n               .setInputCol('widthFeatures')\n               .setOutputCol('widthOneHot'))\n\nassembleOneHot = (VectorAssembler()\n                  .setInputCols(['lengthOneHot', 'widthOneHot'])\n                  .setOutputCol('featuresBucketized'))\n\npipeline.setStages([lengthBucketizer, widthBucketizer, oneHotLength, oneHotWidth, assembleOneHot])\n\ndisplay(pipeline.fit(irisTrain).transform(irisTrain))"],"metadata":{},"outputs":[],"execution_count":57},{"cell_type":"markdown","source":["Create the full `Pipeline` through logistic regression and make predictions on the test data."],"metadata":{}},{"cell_type":"code","source":["pipeline.setStages([lengthBucketizer, widthBucketizer, oneHotLength, oneHotWidth, assembleOneHot, lr])\n\npipelineModelLR2 = pipeline.fit(irisTrain)\n\nirisTestPredictions2 = (pipelineModelLR2\n                        .transform(irisTest)\n                        .cache())\ndisplay(irisTestPredictions2)"],"metadata":{},"outputs":[],"execution_count":59},{"cell_type":"markdown","source":["What does our new model look like?"],"metadata":{}},{"cell_type":"code","source":["logisticModel = pipelineModelLR2.stages[-1]\nprint logisticModel.intercept\nprint repr(logisticModel.weights)"],"metadata":{},"outputs":[],"execution_count":61},{"cell_type":"markdown","source":["What about model accuracy?"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.functions import col\n\ndef modelAccuracy(df):\n  return (df\n          .select((col('prediction') == col('label')).cast('int').alias('correct'))\n          .groupBy()\n          .avg('correct')\n          .first()[0])\n\nmodelOneAccuracy = modelAccuracy(irisTestPredictions)\nmodelTwoAccuracy = modelAccuracy(irisTestPredictions2)\n\nprint 'modelOneAccuracy: {0:.3f}'.format(modelOneAccuracy)\nprint 'modelTwoAccuracy: {0:.3f}'.format(modelTwoAccuracy)"],"metadata":{},"outputs":[],"execution_count":63},{"cell_type":"markdown","source":["Or we can use SQL instead."],"metadata":{}},{"cell_type":"code","source":["irisTestPredictions.registerTempTable('modelOnePredictions')\nsqlResult = sqlContext.sql('select avg(int(prediction == label)) from modelOnePredictions')\ndisplay(sqlResult)"],"metadata":{},"outputs":[],"execution_count":65},{"cell_type":"markdown","source":["An even better option is to use the tools already built-in to Spark.  The MLlib guide has a lot of information regarding [evaluation metrics](http://spark.apache.org/docs/latest/mllib-evaluation-metrics.html).  For ML, you can find details in the [Python](http://spark.apache.org/docs/latest/api/python/pyspark.ml.html#module-pyspark.ml.evaluation) and [Scala](http://spark.apache.org/docs/latest/api/scala/#org.apache.spark.ml.evaluation.package) APIs.\n \nA common metric used for logistic regression is area under the ROC curve (AUC).  We can use the `BinaryClasssificationEvaluator` to obtain the AUC for our two models.  Make sure to set the metric to \"areaUnderROC\" and that you set the rawPrediction column to \"rawPrediction\".\n \nRecall that `irisTestPredictions` are the test predictions from our first model and `irisTestPredictions2` are the test predictions from our second model."],"metadata":{}},{"cell_type":"code","source":["# ANSWER\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\n\nbinaryEvaluator = (BinaryClassificationEvaluator()\n                   .setRawPredictionCol('rawPrediction')\n                   .setMetricName('areaUnderROC'))\n\nfirstModelTestAUC = binaryEvaluator.evaluate(irisTestPredictions)\nsecondModelTestAUC = binaryEvaluator.evaluate(irisTestPredictions2)\n\nprint 'First model AUC: {0:.3f}'.format(firstModelTestAUC)\nprint 'Second model AUC: {0:.3f}'.format(secondModelTestAUC)\n\nirisTrainPredictions = pipelineModelLR.transform(irisTrain)\nirisTrainPredictions2 = pipelineModelLR2.transform(irisTrain)\n\nfirstModelTrainAUC = binaryEvaluator.evaluate(irisTrainPredictions)\nsecondModelTrainAUC = binaryEvaluator.evaluate(irisTrainPredictions2)\n\nprint '\\nFirst model training AUC: {0:.3f}'.format(firstModelTrainAUC)\nprint 'Second model training AUC: {0:.3f}'.format(secondModelTrainAUC)"],"metadata":{},"outputs":[],"execution_count":67},{"cell_type":"code","source":["# TEST\nTest.assertTrue(firstModelTestAUC > .95, 'incorrect firstModelTestAUC')\nTest.assertTrue(secondModelTrainAUC > .95, 'incorrect secondModelTrainAUC')"],"metadata":{},"outputs":[],"execution_count":68},{"cell_type":"markdown","source":["**Visualization: ROC curve **\n \nWe will now visualize how well the model predicts our target.  To do this we generate a plot of the ROC curve.  The ROC curve shows us the trade-off between the false positive rate and true positive rate, as we liberalize the threshold required to predict a positive outcome.  A random model is represented by the dashed line."],"metadata":{}},{"cell_type":"code","source":["import matplotlib.pyplot as plt\nimport matplotlib.cm as cm\nimport numpy as np\n\ndef prepareSubplot(xticks, yticks, figsize=(10.5, 6), hideLabels=False, gridColor='#999999',\n                gridWidth=1.0, subplots=(1, 1)):\n    \"\"\"Template for generating the plot layout.\"\"\"\n    plt.close()\n    fig, axList = plt.subplots(subplots[0], subplots[1], figsize=figsize, facecolor='white',\n                               edgecolor='white')\n    if not isinstance(axList, np.ndarray):\n        axList = np.array([axList])\n\n    for ax in axList.flatten():\n        ax.axes.tick_params(labelcolor='#999999', labelsize='10')\n        for axis, ticks in [(ax.get_xaxis(), xticks), (ax.get_yaxis(), yticks)]:\n            axis.set_ticks_position('none')\n            axis.set_ticks(ticks)\n            axis.label.set_color('#999999')\n            if hideLabels: axis.set_ticklabels([])\n        ax.grid(color=gridColor, linewidth=gridWidth, linestyle='-')\n        map(lambda position: ax.spines[position].set_visible(False), ['bottom', 'top', 'left', 'right'])\n\n    if axList.size == 1:\n        axList = axList[0]  # Just return a single axes object for a regular plot\n    return fig, axList"],"metadata":{},"outputs":[],"execution_count":70},{"cell_type":"code","source":["def generateROC(ax, labelsAndScores):\n    labelsAndWeights = labelsAndScores.collect()\n    labelsAndWeights.sort(key=lambda (k, v): v, reverse=True)\n    labelsByWeight = np.array([0.0] + [k for (k, v) in labelsAndWeights])\n\n    length = labelsByWeight.size - 1\n    truePositives = labelsByWeight.cumsum()\n    numPositive = truePositives[-1]\n    falsePositives = np.arange(0.0, length + 1, 1.) - truePositives\n\n    truePositiveRate = truePositives / numPositive\n    falsePositiveRate = falsePositives / (length - numPositive)\n\n    # Generate layout and plot data\n    ax.set_xlim(-.05, 1.05), ax.set_ylim(-.05, 1.05)\n    ax.set_ylabel('True Positive Rate (Sensitivity)')\n    ax.set_xlabel('False Positive Rate (1 - Specificity)')\n    ax.plot(falsePositiveRate, truePositiveRate, color='red', linestyle='-', linewidth=3.)\n    ax.plot((0., 1.), (0., 1.), linestyle='--', color='orange', linewidth=2.)  # Baseline model\n\n\nlabelsAndScores = (irisTestPredictions\n                   .select('label', 'rawPrediction')\n                   .rdd\n                   .map(lambda r: (r[0], r[1][1])))\n\nlabelsAndScores2 = (irisTestPredictions2\n                    .select('label', 'rawPrediction')\n                    .rdd\n                    .map(lambda r: (r[0], r[1][1])))\n\nfig, axList = prepareSubplot(np.arange(0., 1.1, 0.1), np.arange(0., 1.1, 0.1), figsize=(12., 5.), subplots=(1,2))\nax0, ax1 = axList\nax0.set_title('First Model', color='#999999')\nax1.set_title('Second Model', color='#999999')\ngenerateROC(axList[0], labelsAndScores)\ngenerateROC(axList[1], labelsAndScores2)\ndisplay(fig)"],"metadata":{},"outputs":[],"execution_count":71},{"cell_type":"code","source":["from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n\nmetric = 'precision'\n\nmulticlassEval = MulticlassClassificationEvaluator()\n\nmulticlassEval.setMetricName(metric)\nprint 'Model one {0}: {1:.3f}'.format(metric, multiclassEval.evaluate(irisTestPredictions))\nprint 'Model two {0}: {1:.3f}\\n'.format(metric, multiclassEval.evaluate(irisTestPredictions2))"],"metadata":{},"outputs":[],"execution_count":72},{"cell_type":"code","source":["import inspect\nprint inspect.getsource(MulticlassClassificationEvaluator)"],"metadata":{},"outputs":[],"execution_count":73},{"cell_type":"markdown","source":["#### Using MLlib instead of ML\n \nWe've been using `ml` transformers, estimators, pipelines, and evaluators.  How can we accomplish the same things with MLlib?"],"metadata":{}},{"cell_type":"code","source":["irisTestPredictions.columns"],"metadata":{},"outputs":[],"execution_count":75},{"cell_type":"code","source":["irisTestPredictions.take(1)"],"metadata":{},"outputs":[],"execution_count":76},{"cell_type":"markdown","source":["Pull the data that we need from our `DataFrame` and create `BinaryClassificationMetrics`."],"metadata":{}},{"cell_type":"code","source":["from pyspark.mllib.evaluation import BinaryClassificationMetrics\n\nmodelOnePredictionLabel = (irisTestPredictions\n                           .select('rawPrediction', 'label')\n                           .rdd\n                           .map(lambda r: (float(r[0][1]), r[1])))\n\nmodelTwoPredictionLabel = (irisTestPredictions2\n                           .select('rawPrediction', 'label')\n                           .rdd\n                           .map(lambda r: (float(r[0][1]), r[1])))\n\nmetricsOne = BinaryClassificationMetrics(modelOnePredictionLabel)\nmetricsTwo = BinaryClassificationMetrics(modelTwoPredictionLabel)\n\nprint metricsOne.areaUnderROC\nprint metricsTwo.areaUnderROC"],"metadata":{},"outputs":[],"execution_count":78},{"cell_type":"markdown","source":["To build a logistic regression model with MLlib we'll need the data to be an RDD of `LabeledPoints`.  For testing purposes we'll pull out the label and features into a tuple, since we'll want to make predictions directly on the features and not on a `LabeledPoint`."],"metadata":{}},{"cell_type":"code","source":["from pyspark.mllib.regression import LabeledPoint\n\nirisTrainRDD = (irisTrainPredictions\n                .select('label', 'featuresBucketized')\n                .map(lambda r: LabeledPoint(r[0], r[1]))\n                .cache())\n\nirisTestRDD = (irisTestPredictions\n               .select('label', 'featuresBucketized')\n               .map(lambda r: (r[0], r[1]))\n               .cache())\n\nprint irisTrainRDD.take(2)\nprint irisTestRDD.take(2)"],"metadata":{},"outputs":[],"execution_count":80},{"cell_type":"markdown","source":["Now, we can use MLlib's logistic regression on our `RDD` of `LabeledPoints`.  Note that we'll use `LogisticRegressionWithLBFGS` as it tends to converge faster than `LogisticRegressionWithSGD`."],"metadata":{}},{"cell_type":"code","source":["from pyspark.mllib.classification import LogisticRegressionWithLBFGS\nhelp(LogisticRegressionWithLBFGS)"],"metadata":{},"outputs":[],"execution_count":82},{"cell_type":"code","source":["mllibModel = LogisticRegressionWithLBFGS.train(irisTrainRDD, iterations=1000, regParam=0.0)"],"metadata":{},"outputs":[],"execution_count":83},{"cell_type":"markdown","source":["Let's calculate our accuracy using `RDDs`."],"metadata":{}},{"cell_type":"code","source":["rddPredictions = mllibModel.predict(irisTestRDD.values())\npredictAndLabels = rddPredictions.zip(irisTestRDD.keys())\n\nmllibAccuracy = predictAndLabels.map(lambda (p, l): p == l).mean()\nprint 'MLlib model accuracy: {0:.3f}'.format(mllibAccuracy)\n"],"metadata":{},"outputs":[],"execution_count":85}],"metadata":{"name":"3-pipeline-logistic_answers","notebookId":7781},"nbformat":4,"nbformat_minor":0}
