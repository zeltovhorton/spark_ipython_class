{"cells":[{"cell_type":"markdown","source":["<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-nd/4.0/\"><img alt=\"Creative Commons License\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by-nc-nd/4.0/88x31.png\" /></a><br />This work is licensed under a <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-nd/4.0/\">Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License</a>."],"metadata":{}},{"cell_type":"markdown","source":["# Wikipedia: Exploratory Data Analysis (EDA) using DataFrames\n \nThis lab explores English wikipedia articles using `DataFrames`.  You'll learn about `DataFrame`, `Column`, and `GroupedData` objects and the `functions` package.  After you complete this lab you should be able to use much of the functionality found in Spark SQL and know where to find additional reference material."],"metadata":{}},{"cell_type":"markdown","source":["#### Load the data and start the EDA\n \nWe'll be mostly using functions and objects that are found in Spark SQL.  The [Python](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html) and [Scala](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.package) APIs and the [Spark SQL and DataFrame Guide](http://spark.apache.org/docs/latest/sql-programming-guide.html) are all very useful references."],"metadata":{}},{"cell_type":"markdown","source":["To start, work from the small sample to speed the EDA process."],"metadata":{}},{"cell_type":"code","source":["baseDir = '/mnt/ml-class/'\ndfSmall = sqlContext.read.parquet(baseDir + 'smallwiki.parquet').cache()\nprint dfSmall.count()"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":["Let's take a look at how our `DataFrame` is represented."],"metadata":{}},{"cell_type":"code","source":["print 'dfSmall: {0}'.format(dfSmall)\nprint '\\ntype(dfSmall): {0}'.format(type(dfSmall))"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["print dfSmall.schema, '\\n'\ndfSmall.printSchema()"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":["We can see that our schema is made up of a `StructType` that contains `StructField` objects.  These `StructField` objects have several properties including: name, data type, whether they can be null, and metadata.  Note that the list of fields for a `StructType` can also include other `StructType` objects to allow for nested structures."],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.types import StructField\nhelp(StructField)"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":["Next, we'll create an example `DataFrame` where we specify the schema using `StructType` and `StructField`.  Schema can also be inferred by Spark during `DataFrame` creation."],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.types import StructType, StructField, BooleanType, StringType, LongType\nfrom pyspark.sql import Row\n\nschema = StructType([StructField('title', StringType(), nullable=False, metadata={'language': 'english'}),\n                     StructField('numberOfEdits', LongType()),\n                     StructField('redacted', BooleanType())])\n\nexampleData = sc.parallelize([Row(\"Baade's Window\", 100, False),\n                              Row('Zenomia', 10, True),\n                              Row('United States Bureau of Mines', 5280, True)])\n\nexampleDF = sqlContext.createDataFrame(exampleData, schema)\ndisplay(exampleDF)"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":["Let's view the schema that we created."],"metadata":{}},{"cell_type":"code","source":["print exampleDF.schema\nexampleDF.printSchema()"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":["Our `metadata` for the `title` field has also been captured.  We might create a new `DataFrame` from this `DataFrame` using a transformer and we could pass along or modify this `metadata` in the process."],"metadata":{}},{"cell_type":"code","source":["print exampleDF.schema.fields[0].metadata\nprint exampleDF.schema.fields[1].metadata"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":["What does a row of wikipedia data look like?  Let's take a look at the first observation."],"metadata":{}},{"cell_type":"code","source":["print dfSmall.first()"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":["What are our column names?"],"metadata":{}},{"cell_type":"code","source":["print dfSmall.columns"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":["The text is long and obscures the rest of the data.  Let's use `drop` to remove the text."],"metadata":{}},{"cell_type":"code","source":["print dfSmall.drop('text').first()"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":["Next, let's view the text in a format that more closely resembles how it would be displayed."],"metadata":{}},{"cell_type":"code","source":["print dfSmall.select('text').first()[0]"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"markdown","source":["When we parsed the XML we stored `<PARSE ERROR>` as the title for any record that our XML parser couldn't handle.  Let's see how many records had errors."],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.functions import col\nerrors = dfSmall.filter(col('title') == '<PARSE ERROR>')\nerrorCount = errors.count()\nprint errorCount\nprint errorCount / float(dfSmall.count())"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"markdown","source":["We can also do the `Column` selection several different ways."],"metadata":{}},{"cell_type":"code","source":["print dfSmall.filter(dfSmall['title'] == '<PARSE ERROR>').count()\nprint dfSmall.filter(dfSmall.title == '<PARSE ERROR>').count()"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"markdown","source":["We can see that `errors` contains those items with a title that equals `<PARSE ERROR>`.  Note that we can rename our column using `.alias()` and display our `DataFrame` using `.show()`.  `alias` is a method that we are calling on a `Column` and `show` is a method called on the `DataFrame`."],"metadata":{}},{"cell_type":"code","source":["errors.select(col('title').alias('badTitle')).show(3)"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"markdown","source":["And what does an error look like?"],"metadata":{}},{"cell_type":"code","source":["print errors.select('text').first()[0]"],"metadata":{},"outputs":[],"execution_count":32},{"cell_type":"markdown","source":["Let's use some `Column` and `DataFrame` operations to inspect the `redirect_title` column."],"metadata":{}},{"cell_type":"code","source":["(dfSmall\n .select(col('redirect_title').isNotNull().alias('hasRedirect'))\n .groupBy('hasRedirect')\n .count()\n .show())"],"metadata":{},"outputs":[],"execution_count":34},{"cell_type":"markdown","source":["Now, let's filter out the data that has a parse error, is a redirect, or doesn't have any text."],"metadata":{}},{"cell_type":"code","source":["filtered = dfSmall.filter((col('title') != '<PARSE ERROR>') &\n                           col('redirect_title').isNull() &\n                           col('text').isNotNull())\nprint filtered.count()"],"metadata":{},"outputs":[],"execution_count":36},{"cell_type":"markdown","source":["#### Helpful functions\n \nIn addition to the functions that can be called on a `DataFrame`, `Column`, or `GroupedData`, Spark SQL also has a `functions` package that provides functions like those typically built into a database system that can be called from SQL.  This include functions for performing mathematical operations, handling dates and times, string manipulation, and more.\n \nThe [Python](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.functions) and [Scala](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.functions$) APIs have good descriptions for these functions."],"metadata":{}},{"cell_type":"code","source":["import pyspark.sql.functions as func"],"metadata":{},"outputs":[],"execution_count":38},{"cell_type":"code","source":["dir(func)"],"metadata":{},"outputs":[],"execution_count":39},{"cell_type":"markdown","source":["Next, we'll use the time functions to convert our timestamp into Central European Summer Time (CEST)."],"metadata":{}},{"cell_type":"code","source":["filtered.select('timestamp').show(5)"],"metadata":{},"outputs":[],"execution_count":41},{"cell_type":"markdown","source":["Let's try applying `date_format` to see how it operates."],"metadata":{}},{"cell_type":"code","source":["(filtered\n .select('timestamp', func.date_format('timestamp', 'MM/dd/yyyy').alias('date'))\n .show(5))"],"metadata":{},"outputs":[],"execution_count":43},{"cell_type":"code","source":["withDate = filtered.withColumn('date', func.date_format('timestamp', 'MM/dd/yyyy'))\nwithDate.printSchema()\nwithDate.select('title', 'timestamp', 'date').show(3)"],"metadata":{},"outputs":[],"execution_count":44},{"cell_type":"markdown","source":["It seems like we want a different function for time zone manipulation and to store the object as a timestamp rather than a string.  Let's use `from_utc_timestamp` to get a timestamp object back with the correct time zone."],"metadata":{}},{"cell_type":"code","source":["withCEST = withDate.withColumn('cest_time', func.from_utc_timestamp('timestamp', 'Europe/Amsterdam'))\nwithCEST.printSchema()\n\n(withCEST\n .select('timestamp', 'cest_time')\n .show(3, False))"],"metadata":{},"outputs":[],"execution_count":46},{"cell_type":"markdown","source":["Next, let's convert the text field to lowercase.  We'll use the `lower` function for this."],"metadata":{}},{"cell_type":"code","source":["lowered = withCEST.select('*', func.lower(col('text')).alias('lowerText'))\n\nprint lowered.select('lowerText').first()"],"metadata":{},"outputs":[],"execution_count":48},{"cell_type":"markdown","source":["What columns do we have now?"],"metadata":{}},{"cell_type":"code","source":["print lowered.columns"],"metadata":{},"outputs":[],"execution_count":50},{"cell_type":"markdown","source":["Let's go ahead and drop the columns we don't want and rename `lowerText` to `text`."],"metadata":{}},{"cell_type":"code","source":["parsed = (lowered\n          .drop('text')\n          .drop('timestamp')\n          .drop('date')\n          .withColumnRenamed('lowerText', 'text'))\n\nprint parsed.columns, '\\n\\n'\nprint parsed.select('text').first()"],"metadata":{},"outputs":[],"execution_count":52},{"cell_type":"markdown","source":["Next, let's convert our text into a list of words so that we can perform some analysis at the word level.  For this we will use a feature transformer called `RegexTokenizer` which splits up strings into tokens (words in our case) based on a split pattern.  We'll split our text on anything that matches one or more non-word characters."],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.feature import RegexTokenizer\n\ntokenizer = (RegexTokenizer()\n             .setInputCol('text')\n             .setOutputCol('words')\n             .setPattern('\\\\W+'))\nwordsDF = tokenizer.transform(parsed)"],"metadata":{},"outputs":[],"execution_count":54},{"cell_type":"code","source":["wordsDF.select('words').first()"],"metadata":{},"outputs":[],"execution_count":55},{"cell_type":"markdown","source":["There are some very common words in our list of words which won't be that useful for our later analysis.  We'll create a UDF to remove them.\n \n[StopWordsRemover](http://spark.apache.org/docs/latest/ml-features.html#stopwordsremover) is implemented for Scala but not yet for Python.  We'll use the same [list](http://ir.dcs.gla.ac.uk/resources/linguistic_utils/stop_words) of stop words it uses to build a user-defined function (UDF)."],"metadata":{}},{"cell_type":"code","source":["stopWords = set(sc.textFile('/mnt/ml-class/stop_words.txt').collect())\nprint [word for i, word in zip(range(5), stopWords)]"],"metadata":{},"outputs":[],"execution_count":57},{"cell_type":"markdown","source":["Create our function for removing words."],"metadata":{}},{"cell_type":"code","source":["import re\nstopWordsBroadcast = sc.broadcast(stopWords)\n\ndef keepWord(word):\n    if len(word) < 3:\n        return False\n\n    if word in stopWordsBroadcast.value:\n        return False\n\n    if re.search(re.compile(r'[0-9_]'), word):\n        return False\n\n    return True\n\ndef removeWords(words):\n    return [word for word in words if keepWord(word)]"],"metadata":{},"outputs":[],"execution_count":59},{"cell_type":"markdown","source":["Test the function locally."],"metadata":{}},{"cell_type":"code","source":["removeWords(['test', 'cat', 'do343', '343', 'spark', 'the', 'and', 'hy-phen', 'under_score'])"],"metadata":{},"outputs":[],"execution_count":61},{"cell_type":"markdown","source":["Create a UDF from our function."],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.functions import udf\nfrom pyspark.sql.types import ArrayType, StringType\n\nremoveWordsUDF = udf(removeWords, ArrayType(StringType()))"],"metadata":{},"outputs":[],"execution_count":63},{"cell_type":"markdown","source":["Register this function so that we can call it later from another notebook.  Note that in Scala `register` also returns a `udf` that we can use, so we could have combined the above step into this step."],"metadata":{}},{"cell_type":"code","source":["sqlContext.udf.register('removeWords', removeWords, ArrayType(StringType()))"],"metadata":{},"outputs":[],"execution_count":65},{"cell_type":"markdown","source":["Apply our function to the `wordsDF` `DataFrame`."],"metadata":{}},{"cell_type":"code","source":["noStopWords = (wordsDF\n               .withColumn('noStopWords', removeWordsUDF(col('words')))\n               .drop('words')\n               .withColumnRenamed('noStopWords', 'words'))\n\nnoStopWords.select('words').take(2)"],"metadata":{},"outputs":[],"execution_count":67},{"cell_type":"markdown","source":["We can save our work at this point by writing out a parquet file."],"metadata":{}},{"cell_type":"code","source":["#noStopWords.write.parquet(\"/mnt/ml-class/smallWords.parquet\")"],"metadata":{},"outputs":[],"execution_count":69},{"cell_type":"markdown","source":["What is the `DataFrame` doing in the background?"],"metadata":{}},{"cell_type":"code","source":["print noStopWords.explain(True)"],"metadata":{},"outputs":[],"execution_count":71},{"cell_type":"markdown","source":["Let's cache `noStopWords` as we'll use it multiple times shortly."],"metadata":{}},{"cell_type":"code","source":["noStopWords.cache()"],"metadata":{},"outputs":[],"execution_count":73},{"cell_type":"markdown","source":["Calculate the number of words in `noStopWords`.  Recall that each row contains an array of words.\n \nOne strategy would be to take the length of each row and sum the lengths.  To do this use `functions.size`, `functions.sum`, and call `.agg` on the `DataFrame`.\n \nDon't forget to refer to the  [Python](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html) and [Scala](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.package) APIs.  For example you'll find detail for the function `size` in the [functions module](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.size) in Python and the [functions package](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.functions$) in Scala."],"metadata":{}},{"cell_type":"markdown","source":["First, create a `DataFrame` named sized that has a `size` column with the size of each array of words.  Here you can use `func.size`."],"metadata":{}},{"cell_type":"code","source":["# ANSWER\nsized = noStopWords.withColumn('size', func.size('words'))\n\nsizedFirst = sized.select('size', 'words').first()\nprint sizedFirst[0]"],"metadata":{},"outputs":[],"execution_count":76},{"cell_type":"code","source":["# TEST\nfrom test_helper import Test\nTest.assertEquals(sizedFirst[0], len(sizedFirst[1]), 'incorrect implementation for sized')"],"metadata":{},"outputs":[],"execution_count":77},{"cell_type":"markdown","source":["Next, you'll need to aggregate the counts.  You can do this using `func.sum` in either a `.select` or `.agg` method call on the `DataFrame`.  Make sure to give your `Column` the alias `numberOfWords`.  There are some examples in [Python](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.GroupedData.agg) and [Scala](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.DataFrame) in the APIs."],"metadata":{}},{"cell_type":"code","source":["# ANSWER\nnumberOfWords = sized.agg(func.sum('size').alias('numberOfWords'))\n\nwordCount = numberOfWords.first()[0]\nprint wordCount"],"metadata":{},"outputs":[],"execution_count":79},{"cell_type":"code","source":["# TEST\nTest.assertEquals(wordCount, 1903220, 'incorrect word count')"],"metadata":{},"outputs":[],"execution_count":80},{"cell_type":"markdown","source":["Next, we'll compute the word count using `select`, the function `func.explode()`, and then taking a `count()` on the `DataFrame`.  Make sure to name the column returned by the `explode` function 'word'."],"metadata":{}},{"cell_type":"code","source":["# ANSWER\nwordList = noStopWords.select(func.explode('words').alias('word'))\n\n# Note that we have one word per Row now\nprint wordList.take(3)\nwordListCount = wordList.count()\nprint wordListCount"],"metadata":{},"outputs":[],"execution_count":82},{"cell_type":"code","source":["# TEST\nTest.assertEquals(wordListCount, 1903220, 'incorrect value for wordListCount')"],"metadata":{},"outputs":[],"execution_count":83},{"cell_type":"markdown","source":["For your final task, you'll group by word and count the number of times each word occurs.  Make sure to return the counts in descending order and to call them `counts`.\n \nFor this task, you can use:\n * `DataFrame` operations `groupBy`, `agg`, and `sort`\n * the `Column` operation `alias`\n * functions `func.count` and `func.desc`."],"metadata":{}},{"cell_type":"code","source":["# ANSWER\nwordGroupCount = (wordList\n                  .groupBy('word')  # group\n                  .agg(func.count('word').alias('counts'))  # aggregate\n                  .sort(func.desc('counts')))  #sort\n\nwordGroupCount.take(5)"],"metadata":{},"outputs":[],"execution_count":85},{"cell_type":"code","source":["# TEST\nTest.assertEquals(tuple(wordGroupCount.first()), (u'ref', 29263), 'incorrect counts.')"],"metadata":{},"outputs":[],"execution_count":86},{"cell_type":"markdown","source":["We could also use SQL to accomplish this counting."],"metadata":{}},{"cell_type":"code","source":["wordList.registerTempTable('wordList')"],"metadata":{},"outputs":[],"execution_count":88},{"cell_type":"code","source":["wordGroupCount2 = sqlContext.sql('select word, count(word) as counts from wordList group by word order by counts desc')\nwordGroupCount2.take(5)"],"metadata":{},"outputs":[],"execution_count":89},{"cell_type":"code","source":["%sql\nselect word, count(word) as counts from wordList group by word order by counts desc"],"metadata":{},"outputs":[],"execution_count":90},{"cell_type":"markdown","source":["Finally, let's see how many distinct words we are working with."],"metadata":{}},{"cell_type":"code","source":["distinctWords = wordList.distinct()\ndistinctWords.take(5)"],"metadata":{},"outputs":[],"execution_count":92},{"cell_type":"code","source":["distinctWords.count()"],"metadata":{},"outputs":[],"execution_count":93}],"metadata":{"name":"wiki-eda_answers","notebookId":115256},"nbformat":4,"nbformat_minor":0}
