{"cells":[{"cell_type":"markdown","source":["<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-nd/4.0/\"><img alt=\"Creative Commons License\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by-nc-nd/4.0/88x31.png\" /></a><br />This work is licensed under a <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-nd/4.0/\">Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License</a>."],"metadata":{}},{"cell_type":"markdown","source":["# Wikipedia: ETL\n \nThis lab explains the process that was used to obtain Wikipedia data and transform it into a more usable form for analysis in Spark."],"metadata":{}},{"cell_type":"markdown","source":["#### ETL Background\n \nWikipedia data from the [August 5, 2015](https://dumps.wikimedia.org/enwiki/20150805/) enwiki dump on dumps.wikimedia.org.  Using the file: `enwiki-20150805-pages-articles-multistream.xml.bz2`  The file was uncompressed, parsed to pull out the XML `<page>` tags, and parsed again to retrieve several fields as JSON.  The JSON was stored one JSON string per line so that Spark could easily load the JSON and write out a parquet file.  The resulting parquet file was downsampled to keep the dataset small for the labs."],"metadata":{}},{"cell_type":"code","source":["wikiSample = \"\"\"<mediawiki xmlns=\"http://www.mediawiki.org/xml/export-0.10/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://www.mediawiki.org/xml/export-0.10/ http://www.mediawiki.org/xml/export-0.10.xsd\" version=\"0.10\" xml:lang=\"en\">\n  <siteinfo>\n    <sitename>Wikipedia</sitename>\n    <dbname>enwiki</dbname>\n    <base>https://en.wikipedia.org/wiki/Main_Page</base>\n    <generator>MediaWiki 1.26wmf16</generator>\n    <case>first-letter</case>\n    <namespaces>\n      <namespace key=\"-2\" case=\"first-letter\">Media</namespace>\n      <namespace key=\"-1\" case=\"first-letter\">Special</namespace>\n      ...\n  <page>\n    <title>AccessibleComputing</title>\n    <ns>0</ns>\n    <id>10</id>\n    <redirect title=\"Computer accessibility\" />\n    <revision>\n      <id>631144794</id>\n      <parentid>381202555</parentid>\n      <timestamp>2014-10-26T04:50:23Z</timestamp>\n      <contributor>\n        <username>Paine Ellsworth</username>\n        <id>9092818</id>\n      </contributor>\n      <comment>add [[WP:RCAT|rcat]]s</comment>\n      <model>wikitext</model>\n      <format>text/x-wiki</format>\n      <text xml:space=\"preserve\">#REDIRECT [[Computer accessibility]]{{Redr|move|from CamelCase|up}}</text>\n      <sha1>4ro7vvppa5kmm0o1egfjztzcwd0vabw</sha1>\n    </revision>\n  </page>\n  ...\n</mediawiki>\"\"\"\nimport cgi\ndisplayHTML('<pre>{0}</pre>'.format(cgi.escape(wikiSample, True)))"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["import xml.etree.ElementTree as ET\nfrom xml.etree.ElementTree import ParseError\nfrom pyspark.sql import Row\n\ndef parse_xml_to_dict(xmlString):\n    data = {\"title\": None, \"redirect_title\": None, \"timestamp\": None, \"last_contributor_username\": None, \"text\": None}\n\n    try:\n        root = ET.fromstring(xmlString.encode('utf-8'))\n\n        title = root.find(\"title\")\n        if title is not None:\n            data[\"title\"] = title.text\n        redirect = root.find(\"redirect\")\n        if redirect is not None:\n            data[\"redirect_title\"] = redirect.attrib[\"title\"]\n        revision = root.find(\"revision\")\n        if revision is not None:\n            timestamp = revision.find(\"timestamp\")\n            data[\"timestamp\"] = timestamp.text\n        contributor = revision.find(\"contributor\")\n        if contributor is not None:\n            username = contributor.find(\"username\")\n        if username is not None:\n            data[\"last_contributor_username\"] = username.text\n        text = revision.find(\"text\")\n        if text is not None and text.text is not None:\n            data[\"text\"] = text.text.replace(\"\\\\n\", \" \")\n    except ParseError:\n        data['title'] = '<PARSE ERROR>'\n\n    return data #Row(**dict)"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["import codecs\nimport json\nfrom xml.etree.ElementTree import ParseError\n\nwikiData = codecs.open('/mnt/data_quick/wiki/extract/enwiki-20150805-pages-articles-multistream.xml', 'r', 'utf-8')\njsonData = codecs.open('/mnt/data_quick/wiki/allpages.json', 'w', 'utf-8')\n\npageData = []\narticleCount = 0\npageCount = 0\npageStart = 0\n\nfor i, line in enumerate(wikiData):\n    #if i > 10000:\n    #    break\n\n    if '<page>' in line:\n        pageCount += 1\n\n        if pageCount > 1:\n            print 'unexpected to have pageCount > 1'\n        else:\n            articleCount += 1\n            pageStart = line.index('<page>')\n\n    if pageCount > 0:\n        pageData.append(line[pageStart:])\n\n    if '</page>' in line:\n        pageCount -= 1\n\n        if pageCount == 0:\n            try:\n                fromxml = parse_xml_to_row(u'\\n'.join(pageData))\n            except ParseError:\n                print u'\\n'.join(pageData)\n                break\n\n            json.dump(fromxml, jsonData)\n            jsonData.write('\\n')\n            pageData = []\n\njsonData.close()\nwikiData.close()"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":["Read in a json file and use Spark to output a Parquet file.\n \nParquet files store data by column in a compressed and efficient manner.  More details can be found at [parquet.apache.org](https://parquet.apache.org/documentation/latest/)."],"metadata":{}},{"cell_type":"code","source":["df = sqlContext.read.json(\"/mnt/data_quick/wiki/allpages.json\")\ndf.write.parquet(\"/mnt/data_quick/wiki/allpages.parquet\")"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":["Read back in the data as `df2` so that the more efficient Parquet format will be our starting point for the `DataFrame`."],"metadata":{}},{"cell_type":"code","source":["df2 = sqlContext.read.parquet(\"/mnt/data_quick/wiki/allpages.parquet\")"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":["Save a 1% sample of the data as another Parquet file."],"metadata":{}},{"cell_type":"code","source":["dfOne = df2.sample(False, .01, 2718).coalesce(24)\ndfOne.write.parquet('/mnt/data_quick/wiki/onepercent.parquet')"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":["Save an even smaller sample."],"metadata":{}},{"cell_type":"code","source":["dfSmall = df2.sample(False, .0005, 2718).coalesce(8)\ndfSmall.write.parquet(\"/mnt/data_quick/wiki/smallwiki.parquet\")"],"metadata":{},"outputs":[],"execution_count":14}],"metadata":{"name":"wiki-etl_answers","notebookId":115448},"nbformat":4,"nbformat_minor":0}
