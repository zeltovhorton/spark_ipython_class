{"cells":[{"cell_type":"markdown","source":["<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-nd/4.0/\"><img alt=\"Creative Commons License\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by-nc-nd/4.0/88x31.png\" /></a><br />This work is licensed under a <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-nd/4.0/\">Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License</a>."],"metadata":{}},{"cell_type":"markdown","source":["# Wikipedia: Word2Vec\n \nIn this lab, we'll use `Word2Vec` to create vectors the words found in the Wikipedia dataset.  We'll use `Word2Vec` by passing in a `DataFrame` containing sentences.  We can pass into `Word2Vec` what length of vector to create, with larger vectors taking more time to build.\n \nBe able to convert words into vectors provides us with features that can be used in traditional machine learning algorithms.  These vectors can be used to compare word similarity, sentence similarity, or even larger sections of text."],"metadata":{}},{"cell_type":"markdown","source":["Load the data."],"metadata":{}},{"cell_type":"code","source":["baseDir = '/mnt/ml-class/'\ndfSmall = sqlContext.read.parquet(baseDir + 'smallwiki.parquet')"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["dfSmall.count()"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":["Filter out unwanted data."],"metadata":{}},{"cell_type":"code","source":["import pyspark.sql.functions as func\nfrom pyspark.sql.functions import col\nfiltered = dfSmall.filter((col('title') != '<PARSE ERROR>') &\n                           col('redirect_title').isNull() &\n                           col('text').isNotNull())"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":["Change all text to lower case."],"metadata":{}},{"cell_type":"code","source":["lowered = filtered.select('*', func.lower(col('text')).alias('lowerText'))"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["parsed = (lowered\n          .drop('text')\n          .withColumnRenamed('lowerText', 'text'))"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":["Split the Wikipedia text into sentences."],"metadata":{}},{"cell_type":"code","source":["pattern = r'(\\. |\\n{2,})'\nimport re\nmatches = re.findall(pattern, 'Wiki page. *More information*\\n\\n And a line\\n that continues.')\nprint matches"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["from pyspark.ml.feature import RegexTokenizer\n\ntokenizer = RegexTokenizer(inputCol='text', outputCol='sentences', pattern=pattern)\nsentences = tokenizer.transform(parsed).select('sentences')\ndisplay(sentences)"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["from pyspark.sql import Row\nfrom pyspark.sql.types import StructType, StructField, StringType\n\nsentenceRDD = (sentences\n               .flatMap(lambda r: r[0])\n               .map(lambda x: Row(sentence=x)))\n\nsentenceSchema = StructType([StructField('sentence', StringType())])\nsentence = sqlContext.createDataFrame(sentenceRDD, sentenceSchema)\n\ndisplay(sentence)"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":["Split the sentences into words."],"metadata":{}},{"cell_type":"code","source":["tokenizerWord = RegexTokenizer(inputCol='sentence', outputCol='words', pattern=r'\\W+')\nwords = tokenizerWord.transform(sentence).select('words')\ndisplay(words)"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":["Use our `removeWords` function that we registered in wiki-eda to clean up stop words."],"metadata":{}},{"cell_type":"code","source":["sqlContext.sql('drop table if exists words')\nwords.registerTempTable('words')"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["noStopWords = sqlContext.sql('select removeWords(words) as words from words') #.cache()\ndisplay(noStopWords)"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":["wordVecInput = noStopWords.filter(func.size('words') != 0)\nwordVecInput.count()"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":["Build the `Word2Vec` model.  This take about a minute with two workers."],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.feature import Word2Vec\nword2Vec = Word2Vec(vectorSize=150, minCount=50, inputCol='words', outputCol='result', seed=0)\nmodel = word2Vec.fit(wordVecInput)"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":["Let's see the model in action."],"metadata":{}},{"cell_type":"code","source":["model.findSynonyms('house', 10).collect()"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"code","source":["synonyms = model.findSynonyms('fruit', 10).collect()\n\nfor word, similarity in synonyms:\n    print(\"{}: {}\".format(word, similarity))"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"code","source":["model.findSynonyms('soccer', 10).collect()"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"markdown","source":["How can we calculate similarity between vectors and handle creating a vector for multiple words at once?"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql import Row\ntmpDF = sqlContext.createDataFrame([Row(words=['fruit']),\n                                    Row(words=['flower']),\n                                    Row(words=['fruit', 'flower'])])"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"code","source":["vFruit = model.transform(tmpDF).map(lambda r: r.result).collect()"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"markdown","source":["Let's create a cosine similarity measure."],"metadata":{}},{"cell_type":"code","source":["from numpy.linalg import norm\n\ndef similarity(x, y):\n  return x.dot(y) / (norm(x) * norm(y))\n\nprint similarity(*vFruit[:2])\nprint similarity(*vFruit[1:])"],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"markdown","source":["`Word2Vec` handles multiple words by averaging the vectors."],"metadata":{}},{"cell_type":"code","source":["print vFruit[0][:6]\nprint vFruit[1][:6]\nprint (vFruit[0][:6] + vFruit[1][:6]) / 2  # Averaging the word vectors gives us the vector for both words in a sentence\nprint vFruit[2][:6]"],"metadata":{},"outputs":[],"execution_count":33},{"cell_type":"code","source":["from pyspark.sql import Row\ntmpDF = sqlContext.createDataFrame([Row(words=['king']),\n                                    Row(words=['man']),\n                                    Row(words=['woman']),\n                                    Row(words=['queen'])])\n\nv1 = model.transform(tmpDF).rdd.map(lambda r: r.result).collect()"],"metadata":{},"outputs":[],"execution_count":34},{"cell_type":"code","source":["k, m, w, q = v1\nprint similarity(k, q)\nprint similarity(k, (q + m)/2)\nprint similarity(k, m)\nprint similarity(q, m)\nprint similarity(q, k - m + w)"],"metadata":{},"outputs":[],"execution_count":35}],"metadata":{"name":"wiki-w2v_answers","notebookId":9889},"nbformat":4,"nbformat_minor":0}
