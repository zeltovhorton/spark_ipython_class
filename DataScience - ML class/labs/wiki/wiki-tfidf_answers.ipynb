{"cells":[{"cell_type":"markdown","source":["<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-nd/4.0/\"><img alt=\"Creative Commons License\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by-nc-nd/4.0/88x31.png\" /></a><br />This work is licensed under a <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-nd/4.0/\">Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License</a>."],"metadata":{}},{"cell_type":"markdown","source":["# Wikipedia: TF-IDF with Normalization for K-Means\n \nIn this lab, we explore generating a k-means model to cluster Wikipedia articles.  This clustering could be used as part of an exploratory data analysis (EDA) process or as a way to build features for a supervised learning technique.\n \nWe'll create a `Pipeline` that can be used to make the cluster predictions.  This lab will make use of `RegexTokenizer`, `HashingTF`, `IDF`, `Normalizer`, `Pipeline`, and `KMeans`.  You'll also see how to perform a stratified random sample."],"metadata":{}},{"cell_type":"markdown","source":["Load in the data."],"metadata":{}},{"cell_type":"code","source":["dfSmall = sqlContext.read.load('/mnt/ml-class/smallwiki.parquet')"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":["Filter out non-relevant data."],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.functions import col\nparsed = dfSmall.filter((col('title') != '<PARSE ERROR>') &\n                           col('redirect_title').isNull() &\n                           col('text').isNotNull())\nparsed.take(1)"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":["Use a regular expression to tokenize (split into words).  Pattern defaults to matching the separator, but can be set to match tokens instead."],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.feature import RegexTokenizer\n\ntokenizer = (RegexTokenizer()\n             .setInputCol(\"text\")\n             .setOutputCol(\"words\")\n             .setPattern(\"\\\\W+\"))"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":["Create a `HashingTF` transformer to hash words to buckets with counts, then use an `IDF` estimator to compute inverse-document frequency for buckets based on how frequently words have hashed to those buckets in the given documents.  Next, normalize the tf-idf values so that the \\\\( l^2 \\\\) norm is one for each row."],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.feature import IDF, HashingTF, Normalizer\n\nhashingTF = (HashingTF()\n             .setNumFeatures(10000)\n             .setInputCol(tokenizer.getOutputCol())\n             .setOutputCol('hashingTF'))\n\nidf = (IDF()\n       .setMinDocFreq(10)\n       .setInputCol(hashingTF.getOutputCol())\n       .setOutputCol('idf'))\n\nnormalizer = (Normalizer()\n              .setInputCol(idf.getOutputCol())\n              .setOutputCol('features'))"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":["Now, let's build the `KMeans` estimator and a `Pipeline` that will contain all of the stages.  We'll then call fit on the `Pipeline` which will give us back a `PipelineModel`.  This will take about a minute to run."],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml import Pipeline\nfrom pyspark.ml.clustering import KMeans\n\nkmeans = (KMeans()\n          .setFeaturesCol('features')\n          .setPredictionCol('prediction')\n          .setK(5)\n          .setSeed(0))\n\npipeline = Pipeline().setStages([tokenizer, hashingTF, idf, normalizer, kmeans])\nmodel = pipeline.fit(parsed)"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":["Let's take a look at a sample of the data to see if we can see a pattern between predicted clusters and titles.  We'll use a stratified sample to over-weight the less frequent predictions for inspection purposes."],"metadata":{}},{"cell_type":"code","source":["predictions = model.transform(parsed)\nstratifiedMap = {0: .03, 1: .04, 2: .06, 3: .40, 4: .005}\nsampleDF = predictions.sampleBy('prediction', stratifiedMap, 0)\ndisplay(sampleDF.select('title', 'prediction').orderBy('prediction'))"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["predictions.columns"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["display(predictions.select(\"features\"))"],"metadata":{},"outputs":[],"execution_count":16}],"metadata":{"name":"wiki-tfidf_answers","notebookId":10739},"nbformat":4,"nbformat_minor":0}
