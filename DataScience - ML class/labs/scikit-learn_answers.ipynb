{"cells":[{"cell_type":"markdown","source":["# External modeling libraries\n---\nHow can we leverage our existing experience with modeling libraries like [scikit-learn](http://scikit-learn.org/stable/index.html)?  We'll explore three approaches that make use of existing libraries, but still benefit from the parallelism provided by Spark.\n\nThese approaches are:\n * Grid Search\n * Cross Validation\n * Sampling\n \nWe'll start by using scikit-learn on the driver and then we'll demonstrate the parallel techniques."],"metadata":{}},{"cell_type":"markdown","source":["## Part 1: Use scikit-learn locally"],"metadata":{}},{"cell_type":"markdown","source":["Load the data from `sklearn.datasets`, and create test and train sets."],"metadata":{}},{"cell_type":"code","source":["import numpy as np\nfrom sklearn import datasets"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["# Load the data\niris = datasets.load_iris()\n\n# Generate test and train sets\nsize = len(iris.target)\nindices = np.random.permutation(size)\n\ncutoff = int(size * .30)\n\ntestX = iris.data[indices[0:cutoff],:]\ntrainX = iris.data[indices[cutoff:],:]\ntestY = iris.target[indices[0:cutoff]]\ntrainY = iris.target[indices[cutoff:]]"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":["Build a nearest neighbors classifier using [sklearn.neighbors.KNeighborsClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html)."],"metadata":{}},{"cell_type":"code","source":["from sklearn.neighbors import KNeighborsClassifier\n\n# Create a KNeighborsClassifier using the default settings\nknn = KNeighborsClassifier()\nknn.fit(trainX, trainY)\n\npredictions = knn.predict(testX)\n\n# Print out the accuracy of the classifier on the test set\nprint sum(predictions == testY) / float(len(testY))"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":["## Part 2: Grid Search"],"metadata":{}},{"cell_type":"markdown","source":["Define a function `runNearestNeighbors` that takes in a parameter `k` and returns a tuple of (`k`, accuracy).  Note that we'll load the data from `sklearn.datasets`, and we'll create train and test splits using [sklearn.cross_validation.train_test_split](http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.train_test_split.html)."],"metadata":{}},{"cell_type":"code","source":["from sklearn.cross_validation import train_test_split\n\ndef runNearestNeighbors(k):\n    # Load dataset from sklearn.datasets\n    irisData = datasets.load_iris()\n    \n    # Split into train and test using sklearn.cross_validation.train_test_split\n    yTrain, yTest, XTrain, XTest = train_test_split(irisData.target, \n                                                    irisData.data)\n    \n    # Build the model\n    knn = KNeighborsClassifier(n_neighbors=k)\n    knn.fit(XTrain, yTrain)\n    \n    # Calculate predictions and accuracy\n    predictions = knn.predict(XTest)\n    accuracy = (predictions == yTest).sum() / float(len(yTest))\n    \n    return (k, accuracy)   "],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":["Now we'll run a grid search for `k` from 1 to 10."],"metadata":{}},{"cell_type":"code","source":["k = sc.parallelize(xrange(1, 11))\nresults = k.map(runNearestNeighbors)\nprint '\\n'.join(map(str, results.collect()))"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":["Let's transfer the data using a Broadcast instead of loading it at each executor.  You can create a [Broadcast](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.Broadcast) variable using [sc.broadcast()](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.SparkContext.broadcast)."],"metadata":{}},{"cell_type":"code","source":["# Create the Broadcast variable\nirisBroadcast = sc.broadcast(iris)\n\ndef runNearestNeighborsBroadcast(k):\n    # Using the data in the irisBroadcast variable split into train and test using\n    # sklearn.cross_validation.train_test_split\n    yTrain, yTest, XTrain, XTest = train_test_split(irisBroadcast.value.target,\n                                                    irisBroadcast.value.data)\n    \n    # Build the model\n    knn = KNeighborsClassifier(n_neighbors=k)\n    knn.fit(XTrain, yTrain)\n    \n    # Calculate predictions and accuracy\n    predictions = knn.predict(XTest)\n    accuracy = (predictions == yTest).sum() / float(len(yTest))\n    \n    return (k, accuracy)   \n  \n# Rerun grid search\nk = sc.parallelize(xrange(1, 11))\nresults = k.map(runNearestNeighborsBroadcast)\nprint '\\n'.join(map(str, results.collect()))"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":["### Part 3: Cross Validation"],"metadata":{}},{"cell_type":"markdown","source":["Now we'll use [sklearn.cross_validation.KFold](http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.KFold.html) to evaluate our model using 10-fold cross validation.  First, generate the 10 folds using `KFold`."],"metadata":{}},{"cell_type":"code","source":["from sklearn.cross_validation import KFold\n\n# Create indicies for 10-fold cross validation\nkf = KFold(size, n_folds=10)\n\nfolds = sc.parallelize(kf)\nprint folds.take(1)"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["import matplotlib.pyplot as plt\nimport matplotlib.cm as cm\nimport numpy as np\n\ndef preparePlot(xticks, yticks, figsize=(10.5, 6), hideLabels=False, gridColor='#999999',\n                gridWidth=1.0):\n    \"\"\"Template for generating the plot layout.\"\"\"\n    plt.close()\n    fig, ax = plt.subplots(figsize=figsize, facecolor='white', edgecolor='white')\n    ax.axes.tick_params(labelcolor='#999999', labelsize='10')\n    for axis, ticks in [(ax.get_xaxis(), xticks), (ax.get_yaxis(), yticks)]:\n        axis.set_ticks_position('none')\n        axis.set_ticks(ticks)\n        axis.label.set_color('#999999')\n        if hideLabels: axis.set_ticklabels([])\n    plt.grid(color=gridColor, linewidth=gridWidth, linestyle='-')\n    map(lambda position: ax.spines[position].set_visible(False), ['bottom', 'top', 'left', 'right'])\n    return fig, ax\n\n\nanArray = np.zeros(150)\n\ndata = []\nfor fold in folds.collect():\n  bIdx, rIdx = fold\n  anArray[bIdx] = 0\n  anArray[rIdx] = 1\n  data.append(anArray.copy())\n\ndataValues = np.vstack(data)\n\n# generate layout and plot\nfig, ax = preparePlot(np.arange(-.5, 150, 15), np.arange(-.5, 10, 1), figsize=(8,7), hideLabels=True,\n                      gridColor='#333333', gridWidth=1.1)\nimage = plt.imshow(dataValues,interpolation='nearest', aspect='auto', cmap=cm.winter)\ndisplay(fig) "],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":["Create a function that runs nearest neighbors based on the fold information passed in.  Note that we'll have the function return an [np.array](http://docs.scipy.org/doc/numpy/reference/generated/numpy.array.html) which provides us with additional functionality that we'll take advantage of in a couple steps."],"metadata":{}},{"cell_type":"code","source":["import numpy as np\n\ndef runNearestNeighborsWithFolds((trainIndex, testIndex)):\n    # Assign training and test sets from irisBroadcast using trainIndex and testIndex\n    XTrain = irisBroadcast.value.data[trainIndex]\n    yTrain = irisBroadcast.value.target[trainIndex]\n    XTest = irisBroadcast.value.data[testIndex]\n    yTest = irisBroadcast.value.target[testIndex]\n    \n   # Build the model\n    knn = KNeighborsClassifier(n_neighbors=5)\n    knn.fit(XTrain, yTrain)\n    \n    # Calculate predictions\n    predictions = knn.predict(XTest)\n    \n    # Compute the number of correct predictions and total predictions\n    correct = (predictions == yTest).sum() \n    total = len(testIndex)\n    \n    # Return an np.array of the number of correct predictions and total predictions\n    return np.array([correct, total])"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":["Computer nearest neighbors using each fold."],"metadata":{}},{"cell_type":"code","source":["# Run nearest neighbors on each of the folds\nfoldResults = folds.map(runNearestNeighborsWithFolds)\nprint 'correct / total\\n' + '\\n'.join(map(str, foldResults.collect()))"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":["Now aggregate the results from the folds to see overall accuracy"],"metadata":{}},{"cell_type":"code","source":["# Note that using .sum() on an RDD of numpy arrays sums by columns \ncorrect, total = foldResults.sum()\nprint correct / float(total)"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"markdown","source":["### Part 4: Sampling"],"metadata":{}},{"cell_type":"markdown","source":["We might have a dataset that is too large where we can't use our external modeling library on the full data set.  In this case we might want to build several models on samples of the dataset.  We could either build the same model, using different parameters, or try completely different techniques to see what works best."],"metadata":{}},{"cell_type":"markdown","source":["First we'll parallelize the iris dataset and distributed it across our cluster."],"metadata":{}},{"cell_type":"code","source":["# Split the iris dataset into 8 partitions\nirisData = sc.parallelize(zip(iris.target, iris.data), 8)\nprint irisData.take(2), '\\n'\n\n# View the number of elements found in each of the eight partitions\nprint (irisData\n       .mapPartitions(lambda x: [len(list(x))])\n       .collect())\n\n# View the target (y) stored by partition\nprint '\\n', irisData.keys().glom().collect()"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"code","source":["dataValues = np.vstack([np.array(x[:18]) for x in irisData.keys().glom().collect()])\n\n# generate layout and plot\nfig, ax = preparePlot(np.arange(-.5, 18, 2), np.arange(-.5, 10, 1), figsize=(8,7), hideLabels=True,\n                      gridColor='#555555', gridWidth=1.1)\nimage = plt.imshow(dataValues,interpolation='nearest', aspect='auto', cmap=cm.Blues)\ndisplay(fig) "],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"markdown","source":["Since each of the partitions represents a dataset that we'll be using to run our local model, we have a problem.  The data is ordered, so our partitions are mostly homogenous with regard to our target variable.\n\nWe'll repartition the data using `partitionBy` so that the data is randomly ordered across partitions."],"metadata":{}},{"cell_type":"code","source":["# Randomly reorder the data across partitions\nrandomOrderData = (irisData\n                   .map(lambda x: (np.random.randint(5), x))\n                   .partitionBy(5)\n                   .values())\n\n# Show the new groupings of target variables\nprint randomOrderData.keys().glom().collect()"],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"markdown","source":["Finally, we'll build a function that takes in the target and data from the `randomOrderData` RDD and returns the number of correct and total predictions (with regard to a test set)."],"metadata":{}},{"cell_type":"code","source":["print randomOrderData.keys().mapPartitions(lambda x: [len(list(x))]).collect()"],"metadata":{},"outputs":[],"execution_count":33},{"cell_type":"code","source":["data = []\nfor x in randomOrderData.keys().glom().collect():\n  temp = np.repeat(-1, 35)\n  pData = x[:35]\n  temp[:len(pData)] = pData\n  data.append(temp.copy())\n\ndataValues = np.vstack(data)\n\nfig, ax = preparePlot(np.arange(-.5, 35, 4), np.arange(-.5, 5, 1), figsize=(8,7), hideLabels=True,\n                      gridColor='#555555', gridWidth=1.1)\nimage = plt.imshow(dataValues,interpolation='nearest', aspect='auto', cmap=cm.Blues)\ndisplay(fig) "],"metadata":{},"outputs":[],"execution_count":34},{"cell_type":"code","source":["# Recall what randomOrderData contains\nprint randomOrderData.take(3)"],"metadata":{},"outputs":[],"execution_count":35},{"cell_type":"code","source":["def runNearestNeighborsPartition(labelAndFeatures):\n    y, X = zip(*labelAndFeatures)\n    yTrain, yTest, XTrain, XTest = train_test_split(y, X)\n    \n    knn = KNeighborsClassifier()\n    knn.fit(XTrain, yTrain)\n    \n    predictions = knn.predict(XTest)\n    correct = (predictions == yTest).sum() \n    total = len(yTest)\n    return [np.array([correct, total])]\n\nsampleResults = randomOrderData.mapPartitions(runNearestNeighborsPartition)\n\nprint 'correct / total\\n' + '\\n'.join(map(str, sampleResults.collect()))"],"metadata":{},"outputs":[],"execution_count":36},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":37}],"metadata":{"name":"scikit-learn_answers","notebookId":7707},"nbformat":4,"nbformat_minor":0}
